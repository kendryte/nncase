/* Copyright 2019-2021 Canaan Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#include <nncase/ir/ops/binary.h>
#include <nncase/ir/ops/clamp.h>
#include <nncase/ir/ops/constant.h>
#include <nncase/ir/ops/conv2d.h>
#include <nncase/ir/ops/conv2d_transpose.h>
#include <nncase/ir/visitor.h>
#include <nncase/transforms/neutral/fuse_clamp.h>

using namespace nncase;
using namespace nncase::ir;
using namespace nncase::ir::transforms;

namespace
{
value_range<float> combine(value_range<float> act, constant &c_low, constant &c_high)
{
    auto low = *reinterpret_cast<const float *>(c_low.data().data());
    auto high = *reinterpret_cast<const float *>(c_high.data().data());
    act.min = std::max(act.min, low);
    act.max = std::min(act.max, high);
    return act;
}
}

bool fuse_clamp_conv2d_transform::on_try_match(node &node, transform_context &context)
{
    if (node.runtime_opcode() == op_clamp)
    {
        auto &p = static_cast<clamp &>(node);
        if (auto low = try_get_direct_parent<constant>(p, 1))
        {
            if (auto high = try_get_direct_parent<constant>(p, 2))
            {
                if (auto conv = try_get_direct_parent<conv2d>(p, 0))
                {
                    if (xt::compute_size(low->output().shape()) == 1
                        && xt::compute_size(high->output().shape()) == 1)
                    {
                        context.inputs.emplace_back(&conv->input());
                        context.inputs.emplace_back(&conv->weights());
                        context.inputs.emplace_back(&conv->bias());
                        context.outputs.emplace_back(&p.output());

                        context.matched_nodes.emplace_back(&p);
                        context.matched_nodes.emplace_back(low);
                        context.matched_nodes.emplace_back(high);
                        context.matched_nodes.emplace_back(conv);
                        return true;
                    }
                }
            }
        }
    }

    return false;
}

void fuse_clamp_conv2d_transform::process(transform_context &context)
{
    auto &output = *context.inputs[0]->connection();
    auto &weights = *context.inputs[1]->connection();
    auto &bias = *context.inputs[2]->connection();
    auto inputs = context.outputs[0]->connections();

    auto &old_low = static_cast<constant &>(*context.matched_nodes[1]);
    auto &old_high = static_cast<constant &>(*context.matched_nodes[2]);
    auto &old_conv = static_cast<conv2d &>(*context.matched_nodes[3]);

    auto act = combine(old_conv.fused_activation(), old_low, old_high);

    auto conv = context.graph.emplace<conv2d>(output.shape(), old_conv.weights().shape(), old_conv.groups(),
        old_conv.padding_h(), old_conv.padding_w(), old_conv.stride_h(), old_conv.stride_w(), old_conv.dilation_h(), old_conv.dilation_w(),
        act);
    conv->name(old_conv.name());
    conv->weights().connect(weights);
    conv->bias().connect(bias);

    conv->input().connect(output);
    for (auto &in : dup(inputs))
        in->connect(conv->output());
}

bool fuse_clamp_conv2d_transpose_transform::on_try_match(node &node, transform_context &context)
{
    if (node.runtime_opcode() == op_clamp)
    {
        auto &p = static_cast<clamp &>(node);
        if (auto low = try_get_direct_parent<constant>(p, 1))
        {
            if (auto high = try_get_direct_parent<constant>(p, 2))
            {
                if (auto conv = try_get_direct_parent<conv2d_transpose>(p, 0))
                {
                    if (xt::compute_size(low->output().shape()) == 1
                        && xt::compute_size(high->output().shape()) == 1)
                    {
                        context.inputs.emplace_back(&conv->input());
                        context.inputs.emplace_back(&conv->weights());
                        context.inputs.emplace_back(&conv->bias());
                        context.outputs.emplace_back(&p.output());

                        context.matched_nodes.emplace_back(&p);
                        context.matched_nodes.emplace_back(low);
                        context.matched_nodes.emplace_back(high);
                        context.matched_nodes.emplace_back(conv);
                        return true;
                    }
                }
            }
        }
    }

    return false;
}

void fuse_clamp_conv2d_transpose_transform::process(transform_context &context)
{
    auto &output = *context.inputs[0]->connection();
    auto &weights = *context.inputs[1]->connection();
    auto &bias = *context.inputs[2]->connection();
    auto inputs = context.outputs[0]->connections();

    auto &old_low = static_cast<constant &>(*context.matched_nodes[1]);
    auto &old_high = static_cast<constant &>(*context.matched_nodes[2]);
    auto &old_conv = static_cast<conv2d_transpose &>(*context.matched_nodes[3]);

    auto act = combine(old_conv.fused_activation(), old_low, old_high);

    auto conv = context.graph.emplace<conv2d_transpose>(output.shape(), old_conv.weights().shape(), old_conv.output().shape(), old_conv.groups(),
        old_conv.padding_h(), old_conv.padding_w(), old_conv.output_padding_h(), old_conv.output_padding_w(), old_conv.stride_h(), old_conv.stride_w(), old_conv.dilation_h(), old_conv.dilation_w(),
        act);
    conv->name(old_conv.name());
    conv->weights().connect(weights);
    conv->bias().connect(bias);

    conv->input().connect(output);
    for (auto &in : dup(inputs))
        in->connect(conv->output());
}

bool fuse_clamp_binary_transform::on_try_match(node &node, transform_context &context)
{
    if (node.runtime_opcode() == op_clamp)
    {
        auto &p = static_cast<clamp &>(node);
        if (auto low = try_get_direct_parent<constant>(p, 1))
        {
            if (auto high = try_get_direct_parent<constant>(p, 2))
            {
                if (auto b = try_get_direct_parent<binary>(p, 0))
                {
                    if (xt::compute_size(low->output().shape()) == 1
                        && xt::compute_size(high->output().shape()) == 1)
                    {
                        context.inputs.emplace_back(&b->input_a());
                        context.inputs.emplace_back(&b->input_b());
                        context.outputs.emplace_back(&p.output());

                        context.matched_nodes.emplace_back(&p);
                        context.matched_nodes.emplace_back(low);
                        context.matched_nodes.emplace_back(high);
                        context.matched_nodes.emplace_back(b);
                        return true;
                    }
                }
            }
        }
    }

    return false;
}

void fuse_clamp_binary_transform::process(transform_context &context)
{
    auto &output_a = *context.inputs[0]->connection();
    auto &output_b = *context.inputs[1]->connection();
    auto inputs = context.outputs[0]->connections();

    auto &old_low = static_cast<constant &>(*context.matched_nodes[1]);
    auto &old_high = static_cast<constant &>(*context.matched_nodes[2]);
    auto &old_b = static_cast<binary &>(*context.matched_nodes[3]);

    auto act = combine(old_b.fused_activation(), old_low, old_high);

    auto b = context.graph.emplace<binary>(old_b.binary_op(), output_a.shape(), output_b.shape(), act);
    b->name(old_b.name());

    b->input_a().connect(output_a);
    b->input_b().connect(output_b);
    for (auto &in : dup(inputs))
        in->connect(b->output());
}
